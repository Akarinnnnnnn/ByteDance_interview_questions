# 机器学习面试题整理

概率题另参见 https://www.jianshu.com/p/83147d263ad1?utm_campaign=haruki&utm_content=note&utm_medium=reader_share&utm_source=weixin&from=message&isappinstalled=0



【十大经典数据挖掘算法】系列

1. 决策树 考的比较少了

2. [K-Means](http://www.cnblogs.com/en-heng/p/5173704.html)

3. [SVM](http://www.cnblogs.com/en-heng/p/5965438.html)

4. [Apriori]

5. [EM](http://www.cnblogs.com/en-heng/p/5994192.html)

6. [PageRank](http://www.cnblogs.com/en-heng/p/6124526.html) 做搜索用 原理要知道

7. [AdaBoost](http://www.cnblogs.com/en-heng/p/5974371.html)

8. [kNN](http://www.cnblogs.com/en-heng/p/5000628.html)

9. [Naïve Bayes](http://www.cnblogs.com/en-heng/p/5002753.html)

   

整理到 牛客笔记19页

## 统计与数据处理

什么是似然？ 喜马拉雅

为什么数据量多了方差就变小 字节跳动

softmax与cross entropy的推导，过拟合与正则化，BiLSTM，Gradient Explosion，Top N，特征选择 字节

## 机器学习基础



3、交叉熵公式推导 字节跳动

标签： 决策树
参考回答：

正则化常用的方法？为什么L1 正则化能够使得模型的特征权值变稀疏？小红书

L1 L2正则的区别 字节跳动

AUC值的含义是什么？它越靠近什么代表模型越好？小红书

1.给你M个正样本，N个负样本，以及他们的预测值P，求AUC。（写完之后接问：AUC究竟在衡量模型什么能力？如果现在所有预测值都*1.2，AUC是否会变化？）AUC手撕代码 字节跳动

  这一题印象深刻是因为平时在计算auc的时候，很多同学都知道是roc曲线的面积，但是对auc具体的含义了解不多。 

如果随机删除90%的正样本，那么AUC值（或ROC曲线）会如何变化？小红书

softmax推导 字节跳动

过拟合 字节跳动

## 优化算法

## 决策树



## SVM

SVM判别函数？ SVM对偶形式的推导？(哈啰)

6、SVM 中什么时候用线性核什么时候用高斯核

考点: SVM 应用

详情： 支持向量机
公司: 阿里菜鸟
参考回答：
当数据的特征提取的较好, 所包含的信息量足够大, 很多问题是线性可分的那么可以采用线
性核。 若特征数较少, 样本数适中, 对于时间不敏感, 遇到的问题是线性不可分的时候可以使用高
斯核来达到更好的效果。

7、什么是支持向量机, SVM 与 LR 的区别?

考点: SVM 基础
详情: 支持向量机
参考回答： 支持向量机为一个二分类模型, 它的基本模型定义为特征空间上的间隔最大的线
性分类器。 而它的学习策略为最大化分类间隔, 最终可转化为凸二次规划问题求解。
LR 是参数模型, SVM 为非参数模型。 LR 采用的损失函数为 logistical loss, 而 SVM 采用的是
hinge loss。 在学习分类器的时候, SVM 只考虑与分类最相关的少数支持向量点。 LR 的模型相对
简单, 在进行大规模线性分类时比较方便。

1） LR 是参数模型， SVM 是非参数模型。 

2） 从目 标函数来看， 区别在于逻辑回归采用的是
logistical loss， SVM 采用的是 hinge loss. 这两个损失函数的目 的都是增加对分类影响较大的
数据点的权重， 减少与分类关系较小的数据点的权重。 

3） SVM 的处理方法是只考虑 support
vectors， 也就是和分类最相关的少数点， 去学习分类器。 而逻辑回归通过非线性映射， 大大减
小了离分类平面较远的点的权重， 相对提升了与分类最相关的数据点的权重。

 4） 逻辑回归相对
来说模型更简单， 好理解， 特别是大规模线性分类时比较方便。 而 SVM 的理解和优化相对来说复
杂一些， SVM 转化为对偶问题后, 分类只需要计算与少数几个支持向量的距离, 这个在进行复杂核
函数计算时优势很明显, 能够大大简化模型和计算。 

5） logic 能做的 svm 能做， 但可能在准确
率上有问题， svm 能做的 logic 有的做不了。

11、SVM 的作用， 基本实现原理

知识点： SVM
详情： 支持向量机
参考回答： SVM 可以用于解决二分类或者多分类问题， 此处以二分类为例。 SVM 的目 标是寻
找一个最优化超平面在空间中分割两类数据， 这个最优化超平面需要满足的条件是： 离其最近的
点到其的距离最大化， 这些点被称为支持向量。

解析： 建议练习推导 SVM， 从基本式的推导， 到拉格朗日 对偶问题。

12、SVM 的硬间隔， 软间隔表达式

知识点： SVM
详情： 支持向量机

解析： 不同点在于有无引 入松弛变量

20、SVM 的损失函数

hinge-loss

## NB

9、朴素贝叶斯（naive Bayes） 法的要求是？

知识点： NB
详情： 朴素贝叶斯（NLP 中常用）
参考回答：  贝叶斯定理、 特征条件独立假设
解析： 朴素贝叶斯属于生成式模型， 学习输入和输出的联合概率分布。 给定输入 x， 利用贝
叶斯概率定理求出最大的后验概率作为输出 y。

## Logistic Regression

4、LR 公式、推导及其损失函数

标签： 逻辑回归
参考回答：

LR损失函数的推导？哈啰出行

4.1、LR 和线性回归的区别

考点: 模型比较
详情： 逻辑回归（工业界常用）
参考回答：
线性回归用来做预测, LR 用来做分类。 线性回归是来拟合函数, LR 是来预测函数。 线性回归
用最小二乘法来计算参数, LR 用最大似然估计来计算参数。 线性回归更容易受到异常值的影响,
而 LR 对异常值有较好的稳定性。

5、逻辑回归怎么实现多分类 通联数据

考点: 逻辑回归的应用
详情： 逻辑回归
参考回答：
方式一: 修改逻辑回归的损失函数, 使用 softmax 函数构造模型解决多分类问题, softmax 分
类模型会有相同于类别数的输出, 输出的值为对于样本属于各个类别的概率, 最后对于样本进行
预测的类型为概率值最高的那个类别。
方式二: 根据每个类别都建立一个二分类器, 本类别的样本标签定义为 0, 其它分类样本标签
定义为 1, 则有多少个类别就构造多少个逻辑回归分类器
若所有类别之间有明显的互斥则使用 softmax 分类器, 若所有类别不互斥有交叉的情况则构
造相应类别个数的逻辑回归分类器。

## 感知机

## PCA

1、**讲一下 PCA**

考点: 降维技术
参考回答：
PCA 是比较常见的线性降维方法, 通过线性投影将高维数据映射到低维数据中, 所期望的是
在投影的维度上, 新特征自 身的方差尽量大, 方差越大特征越有效, 尽量使产生的新特征间的相关
性越小。
PCA 算法的具体操作为对所有的样本进行中心化操作, 计算样本的协方差矩阵, 然后对协方
差矩阵做特征值分解, 取最大的 n 个特征值对应的特征向量构造投影矩阵。

2、PCA 中第一主成分是第一的原因

考点: 降维技术
详情： 特征值分解
参考回答：
略

## LDA

## SVD

## HMM

HMM是什么，解释一下？通联数据

## CRF

## Pagerank

PageRank是什么，能讲一下吗？它的公式是什么？通联

## 聚类

### K-means



## 集成学习

Bagging和boosting的区别？哈啰/甜橙金融

### Adaboost

### RF

随机森林能讲讲吗？第四范式

随机森林怎么进行特征的选取？ akuna

### GBDT

### XGBoost













8、机器学习中的距离计算方法?

欧式距离曼哈顿距离余弦距离切比雪夫距离



10、训练集中类别不均衡， 哪个参数最不准确？

知识点： 类别不平衡、 评价指标的了解
详情： 随机森林
参考回答： 准确度（Accuracy）
解析： 举例， 对于二分类问题来说， 正负样例比相差较大为 99: 1， 模型更容易被训练成预
测较大占比的类别。 因为模型只需要对每个样例按照 0. 99 的概率预测正类， 该模型就能达到 99%
的准确率。



13、如果给你一些数据集， 你会如何分类（我是分情况答的， 从数据的大小， 特征，
是否有缺失， 分情况分别答的）；

知识点： 分类

详情： 逻辑回归（工业界常用）
答案： 根据数据类型选择不同的模型， 如 Lr 或者 SVM， 决策树。 假如特征维数较多， 可以
选择 SVM 模型， 如果样本数量较大可以选择 LR 模型， 但是 LR 模型需要进行数据预处理； 假如缺
失值较多可以选择决策树。 选定完模型后， 相应的目 标函数就确定了。 还可以在考虑正负样例比
比， 通过上下集采样平衡正负样例比。
解析： 需要了解多种分类模型的优缺点， 以及如何构造分类模型的步骤

14、如果数据有问题， 怎么处理；

知识点： 实践问题
详情： 随机森林
答案： 1. 上下采样平衡正负样例比； 2. 考虑缺失值； 3. 数据归一化
解析： 发散问题需要自 己展现自 己的知识面

15、分层抽样的适用范围

考点: 抽样调查
详情： 随机森林
参考回答：
分层抽样利用事先掌握的信息, 充分考虑了保持样本结构和总体结构的一致性, 当总体由差
异明显的几部分组成的时候, 适合用分层抽样。

16、生成模型和判别模型基本形式， 有哪些？

参考回答：
生成式： 朴素贝叶斯、 HMM、 Gaussians、 马尔科夫随机场
判别式： LR， SVM， 神经网络， CRF， Boosting
详情： 支持向量机

17、核函数的种类和应用场景

线性核、 多项式核、 高斯核。
特征维数高选择线性核
样本数量可观、 特征少选择高斯核（非线性核）
样本数量非常多选择线性核（避免造成庞大的计算量）

当样本的特征很多且维数很高时可考虑用 SVM 的线性核函数。 当样本的数量较多, 特征较少
时, 一般手动进行特征的组合再使用 SVM 的线性核函数。 当样本维度不高且数量较少时, 且不知道
该用什么核函数时一般优先使用高斯核函数, 因为高斯核函数为一种局部性较强的核函数, 无论
对于大样本还是小样本均有较好的性能且相对于多项式核函数有较少的参数。详情： 支持向量机

到第28页

18、分类算法列一下有多少种？ 应用场景。

参考回答：
单一的分类方法主要包括： LR 逻辑回归， SVM 支持向量机， DT 决策树、 NB 朴素贝叶斯、 NN
人工神经网络、 K-近邻； 集成学习算法： 基于 Bagging 和 Boosting 算法思想， RF 随机森林, GBDT，
Adaboost, XGboost。  
19、给你一个检测的项目 ， 检测罐装的可口可乐， 瓶装的可口可乐作为负样本， 怎么弄？

参考回答：
二分类问题， 两类标签： 罐装和瓶装， 特征工程、 构建特征； 选择模型（LR, SVM, 决策树等）、
输入模型训练。 考虑下正负样例比。



21、核函数的作用

核函数隐含着一个从低维空间到高维空间的映射, 这个映射可以把低维空间中线性不可分的
两类点变成线性可分的。

22、SVM 为什么使用对偶函数求解

对偶将原始问题中的约束转为了对偶问题中的等式约束, 而且更加方便了核函数的引 入, 同
时也改变了问题的复杂度, 在原始问题下, 求解问题的复杂度只与样本的维度有关, 在对偶问题下,
只与样本的数量有关。

23、ID3,C4.5 和 CART 三种决策树的区别

ID3 决策树优先选择信息增益大的属性来对样本进行划分, 但是这样的分裂节点方法有一个
很大的缺点, 当一个属性可取值数目 较多时, 可能在这个属性对应值下的样本只有一个或者很少
个, 此时它的信息增益将很高, ID3 会认为这个属性很适合划分, 但实际情况下叫多属性的取值会
使模型的泛化能力较差, 所以 C4. 5 不采用信息增益作为划分依据, 而是采用信息增益率作为划分
依据。 但是仍不能完全解决以上问题, 而是有所改善, 这个时候引 入了 CART 树, 它使用 gini 系数
作为节点的分裂依据。

24、SVM 和全部数据有关还是和局部数据有关

SVM 只和分类界限上的支持向量点有关, 换而言之只和局部数据有关。

25、为什么高斯核能够拟合无穷维度

因为将泰勒展开式代入高斯核, 将会得到一个无穷维度的映射

26、完整推导了 svm 一遍， 还有强化学习问的很多， dqn 的各种 trick 了解多少， 怎么实现
知不知道。

SVM 推导：
支持向量机是一种二分类模型， 他的基本想法就是基于训练集和样本空间中找到一个最好的
划分超平面， 将两类样本分割开来， 首先你就要知道什么样的划分发才能称为“最” 好划分

看上图， 二维平面上有两类样本， 一类是用‘+’ 表示， 另一类用‘-’ 表示， 那么中间
那几条划分线每条都能将两类样本分割开来， 但我们我们一眼就注意到中间那条加粗的划分超平
面， 似乎他是最好的， 因为两类的样本点都离他挺远的， 专业点说就是该划分超平面对训练样本
局部扰动的‘容忍’ 性最好。 好， 这还只是个二维平面， 我们可以通过可视化大概寻找这样一个
超平面， 但如果三维， 四维， 五维呢， 我们必须用我们擅长的数学去描述它， 推导它。

。。。。。。

27、SVM 所有核函数的了解应用， SVM 的损失函数

SVM 核函数：
1 核函数本质
核函数的本质可以概括为如下三点：
1） 实际应用中， 常常遇到线性不可分的情况 。 针对这种情况， 常用做法是把样例特征映
射到高维空间中， 转化为线性可分问题。
2） 将样例特征映射到高维空间， 可能会遇到维度过高的问题 。
3） 针对可能的维灾难， 可以利用核函数。 核函数也是将特征从低维到高维的转换， 但避免
了直接进行高维空间中的复杂计算， 可以在低维上进行计算， 却能在实质上将分类效果表现在
高维上。
当然， SVM 也能处理线性可分问题， 这时使用的就是线性核了。
常用的核函数包括如下几个： 线性核函数， 多项式核函数， RBF 核函数(高斯核) ， Sigmoid
核函数

28、朴素贝叶斯基本原理和预测过程

29、LR 推导、公式

30、交叉熵

31、L1 和 L2 正则化的区别

考点: 防过拟合
详情： 线性回归
参考回答：
L1 是模型各个参数的绝对值之和, L2 为各个参数平方和的开方值。 L1 更趋向于产生少量的
特征, 其它特征为 0, 最优的参数值很大概率出现在坐标轴上, 从而导致产生稀疏的权重矩阵, 而
L2 会选择更多的矩阵, 但是这些矩阵趋向于 0。

32、Loss Function 有哪些， 怎么用？

知识点： 损失函数
详情： 线性回归
答案： 平方损失（预测问题） 、 交叉熵（分类问题） 、 hinge 损失（SVM 支持向量机） 、 CART
回归树的残差损失

33、线性回归的表达式， 损失函数

知识点： 线性回归
答案： 线性回归 y=wx+b， w 和 x 可能是多维。 线性回归的损失函数为平方损失函数。
解析： 一般会要求反向求导推导

34、知道哪些传统机器学习模型

1） . 回归算法： 回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。 回归
算法是统计机器学习的利器。 常见的回归算法包括： 最小二乘法（Ordinary Least Square） ，
逻辑回归（Logistic Regression） ， 逐步式回归（Stepwise Regression） ， 多元自 适应回归样
条（Multivariate Adaptive Regression Splines） 以及本地散点平滑估计（Locally Estimated
Scatterplot Smoothing） 。
2） . 基于实例的算法： 基于实例的算法常常用来对决策问题建立模型， 这样的模型常常先选
取一批样本数据， 然后根据某些近似性把新数据与样本数据进行比较。 通过这种方式来寻找最佳
的匹配。 因此， 基于实例的算法常常也被称为“赢家通吃” 学习或者“基于记忆的学习” 。 常见
的算法包括 k-Nearest Neighbor(KNN) , 学习矢量量化（Learning Vector Quantization， LVQ），
以及自 组织映射算法（Self-Organizing Map， SOM） 。 深度学习的概念源于人工神经网络的研究。
含多隐层的多层感知器就是一种深度学习结构。 深度学习通过组合低层特征形成更加抽象的高层
表示属性类别或特征， 以发现数据的分布式特征表示。
3） . 决策树学习： 决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常
常用来解决分类和回归问题。 常见的算法包括： 分类及回归树（Classification And Regression
Tree， CART） ， ID3 (Iterative Dichotomiser 3) ， C4. 5， Chi-squared Automatic Interaction
Detection(CHAID) , Decision Stump, 随机森林（Random Forest） ， 多元自 适应回归样条（MARS）
以及梯度推进机（Gradient Boosting Machine， GBM） 。
4） . 贝叶斯方法： 贝叶斯方法算法是基于贝叶斯定理的一类算法， 主要用来解决分类和回归
问题。 常见算法包括： 朴素贝叶斯算法， 平均单依赖估计（Averaged One-Dependence Estimators，
AODE） ， 以及 Bayesian Belief Network（BBN） 。
5） . 基于核的算法： 基于核的算法中最著名的莫过于支持向量机（SVM） 了。 基于核的算法
把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更
容易的解决。 常见的基于核的算法包括： 支持向量机（Support Vector Machine， SVM） ， 径向
基函数（Radial Basis Function， RBF) ， 以及线性判别分析（Linear Discriminate Analysis，
LDA) 等。
6） . 聚类算法： 聚类， 就像回归一样， 有时候人们描述的是一类问题， 有时候描述的是一类
算法。 聚类算法通常按照中心点或者分层的方式对输入数据进行归并。 所以的聚类算法都试图找
到数据的内在结构， 以便按照最大的共同点将数据进行归类。 常见的聚类算法包括 k-Means 算
法以及期望最大化算法（Expectation Maximization， EM） 。
7） . 降低维度算法： 像聚类算法一样， 降低维度算法试图分析数据的内在结构， 不过降低维
度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。 这类算法可以用于高维
数据的可视化或者用来简化数据以便监督式学习使用。 常见的算法包括： 主成份分析（Principle
Component Analysis， PCA） ， 偏最小二乘回归（Partial Least Square Regression， PLS） ，
Sammon 映射， 多维尺度（Multi-Dimensional Scaling, MDS） , 投影追踪（Projection Pursuit）
等。
8） . 关联规则学习： 关联规则学习通过寻找最能够解释数据变量之间关系的规则， 来找出大
量多元数据集中有用的关联规则。 常见算法包括 Apriori 算法和 Eclat 算法等。

9） . 集成算法： 集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练， 然后把
结果整合起来进行整体预测。 集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及
如何把学习结果整合起来。 这是一类非常强大的算法， 同时也非常流行。 常见的算法包括：
Boosting， Bootstrapped Aggregation（Bagging）， AdaBoost， 堆叠泛化（Stacked Generalization，
Blending） ， 梯度推进机（Gradient Boosting Machine, GBM） ， 随机森林（Random Forest） 。
10） . 人工神经网络： 人工神经网络算法模拟生物神经网络， 是一类模式匹配算法。 通常用
于解决分类和回归问题。 人工神经网络是机器学习的一个庞大的分支， 有几百种不同的算法。（其
中深度学习就是其中的一类算法， 我们会单独讨论） ， 重要的人工神经网络算法包括： 感知器神
经网络（Perceptron Neural Network） , 反向传递（Back Propagation） ， Hopfield 网络， 自
组织映射（Self-Organizing Map, SOM） 。 学习矢量量化（Learning Vector Quantization， LVQ）。

35、什么是 DBSCAN

考点: DBSCAN 的原理
详情: 基于密度聚类
DBSCAN 是一种基于密度的空间聚类算法, 它不需要定义簇的个数, 而是将具有足够高密度的
区域划分为簇, 并在有噪声的数据中发现任意形状的簇, 在此算法中将簇定义为密度相连的点的
最大集合。

36、k-means 算法流程

考点: 聚类算法
详情： K 均值
参考回答：
从数据集中随机选择 k 个聚类样本作为初始的聚类中心, 

然后计算数据集中每个样本到这 k个聚类中心的距离, 并将此样本分到距离最小的聚类中心所对应的类中。 

将所有样本归类后, 对于每个类别重新计算每个类别的聚类中心即每个类中所有样本的质心, 重复以上操作直到聚类中心
不变为止。

37、LDA 的原理

LDA 是一种基于有监督学习的降维方式, 将数据集在低维度的空间进行投影, 要使得投影后
的同类别的数据点间的距离尽可能的靠近, 而不同类别间的数据点的距离尽可能的远。

38、KMeans 讲讲， KMeans 有什么缺点， K 怎么确定

在 k-means 算法中， 用质心来表示 cluster； 且容易证明 k-means 算法收敛等同于所有质心
不再发生变化。 基本的 k-means 算法流程如下：
选取 k 个初始质心（作为初始 cluster） ；
repeat： 对每个样本点， 计算得到距其最近的质心， 将其类别标为该质心所对应的 cluster；
重新计算 k 个 cluser 对应的质心；
until 质心不再发生变化
k-means 存在缺点：
1） k-means 是局部最优的， 容易受到初始质心的影响； 比如在下图中， 因选择初始质心不
恰当而造成次优的聚类结果。
2） 同时， k 值的选取也会直接影响聚类结果， 最优聚类的 k 值应与样本数据本身的结构信
息相吻合， 而这种结构信息是很难去掌握， 因此选取最优 k 值是非常困难的。
K 值得确定：
法 1： (轮廓系数) 在实际应用中， 由于 Kmean 一般作为数据预处理， 或者用于辅助分聚类贴
标签。 所以 k 一般不会设置很大。 可以通过枚举， 令 k 从 2 到一个固定值如 10， 在每个 k 值上
重复运行数次 kmeans(避免局部最优解) ， 并计算当前 k 的平均轮廓系数， 最后选取轮廓系数最
大的值对应的 k 作为最终的集群数目 。
法 2： (Calinski-Harabasz 准则)
其中 SSB 是类间方差， ， m 为所有点的中心点, mi 为某类的中心点；
SSW 是类内方差， ；
(N-k) /(k-1) 是复杂度；
比率越大， 数据分离度越大。

至p58

跳过

至p66

39、bagging 和 boosting 的区别

考点: 模型集成的应用
详情： Bagging
参考回答：
Bagging 是从训练集中进行子抽样组成每个基模型所需要的子训练集, 然后对所有基模型预
测的结果进行综合操作产生最终的预测结果。
Boosting 中基模型按次序进行训练, 而基模型的训练集按照某种策略每次都进行一定的转
化, 最后以一定的方式将基分类器组合成一个强分类器。
Bagging 的训练集是在原始集中有放回的选取, 而 Boosting 每轮的训练集不变, 只是训练集
中的每个样本在分类器中的权重都会发生变化, 此权值会根据上一轮的结果进行调整。
Bagging 的各个预测函数可以并行生成, Boosting 的各预测函数只能顺序生成。
Bagging 中整体模型的期望近似于基模型的期望, 所以整体模型的偏差相似于基模型的偏差,
因此 Bagging 中的基模型为强模型(强模型拥有低偏差高方差) 。
Boosting 中的基模型为弱模型, 若不是弱模型会导致整体模型的方差很大。

1） 样本选择上： Bagging： 训练集是在原始集中有放回选取的， 从原始集中选出的各轮训练
集之间是独立的。 Boosting： 每一轮的训练集不变， 只是训练集中每个样例在分类器中的权重发
生变化。 而权值是根据上一轮的分类结果进行调整。
2） 样例权重： Bagging： 使用均匀取样， 每个样例的权重相等。 Boosting： 根据错误率不断
调整样例的权值， 错误率越大则权重越大。

3） 预测函数： Bagging： 所有预测函数的权重相等。 Boosting： 每个弱分类器都有相应的权
重， 对于分类误差小的分类器会有更大的权重。
4） 并行计算： Bagging： 各个预测函数可以并行生成。 Boosting： 各个预测函数只能顺序生
成， 因为后一个模型参数需要前一轮模型的结果。

1） 取样方式（样本权重） ： Bagging 是均匀选取， 样本的权重相等， Boosting 根据错误率
取样， 错误率越大则权重越大。 2） 训练集的选择： Bagging 随机选择训练集， 训练集之间相互
独立， Boosting 的各轮训练集的选择与前面各轮的学习结果有关。 3） 预测函数： Bagging 各个
预测函数没有权重， 可以并行生成， Boosting 有权重， 顺序生成。 4） Bagging 是减少 variance，
Boosting 是减少 bias。
Bagging 是 Bootstrap Aggregating 的简称， 意思就是再取样 (Bootstrap) 然后在每个样
本上训练出来的模型取平均， 所以是降低模型的 variance. Bagging 比如 Random Forest 这种
先天并行的算法都有这个效果。

Boosting 则是迭代算法， 每一次迭代都根据上一次迭代的预测结果对样本进行加权， 所以
随着迭代不不断进行行， 误差会越来越小， 所以模型的 bias 会不不断降低。 这种算法无法并行。

40、XGBOOST 和 GDBT 的区别

考点: 集成模型的基础
详情： GBDT
参考回答：

41、GDBT 的原理, 以及常用的调参参数

考点: GDBT 知识点
详情： GBDT
参考回答：
先用一个初始值去学习一棵树, 然后在叶子处得到预测值以及预测后的残差, 之后的树则基
于之前树的残差不断的拟合得到, 从而训练出一系列的树作为模型。
n_estimators 基学习器的最大迭代次数, learning_rate 学习率， max_lead_nodes 最大叶子
节点数, max_depth 树的最大深度, min_samples_leaf 叶子节点上最少样本数。

42、stacking 和 blending 的区别?

考点: 模型集成
详情： Bagging
参考回答：
Stacking和 blending 的区别在于数据的划分, blending用不相交的数据训练不同的基模型,
并将其输出取加权平均。 而 stacking 是将数据集划分为两个不相交的集合, 在第一个集合的数据集中训练多个模型, 在第二个数据集中测试这些模型, 将预测结果作为输入, 将正确的标签作为输
出, 再训练一个高层的模型。

43、AdaBoost 和 GBDT 的区别

考点: Boosting
详情： AdaBoost
参考回答：
AdaBoost 通过调整错分的数据点的权重来改进模型, 而 GBDT 是从负梯度的方向去拟合改进
模型。
AdaBoost 改变了训练数据的权值, 即样本的概率分布, 减少上一轮被正确分类的样本权值,
提高被错误分类的样本权值, 而随机森林在训练每棵树的时候, 随机挑选部分训练集进行训练。 在
对新数据进行预测时, AdaBoost 中所有树加权投票进行预测, 每棵树的权重和错误率有关, 而随
机森林对所有树的结果按照少数服从多数的原则进行预测。

44、gbdt 推导、适用场景、算法过程

45、rf 和 gbdt 基分类器区别， 里面的决策树分别长啥样， 怎么剪枝

46、xgboost 的特征重要性计算

考点: xgboost 基础
参考回答:
Xgboost 根据结构分数的增益情况计算出来选择哪个特征作为分割点, 而某个特征的重要性
就是它在所有树中出现的次数之和。

47、xgboost 的正则项表达式

48、xgboost 原理， 怎么防过拟合

49、xgboost， rf， lr 优缺点场景

Xgboost：
优缺点： 1） 在寻找最佳分割点时， 考虑传统的枚举每个特征的所有可能分割点的贪心法效
率太低， xgboost 实现了一种近似的算法。 大致的思想是根据百分位法列举几个可能成为分割点
的候选者， 然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。 2） xgboost 考虑
了训练数据为稀疏值的情况， 可以为缺失值或者指定的值指定分支的默认方向， 这能大大提升算
法的效率， paper 提到 50 倍。 3） 特征列排序后以块的形式存储在内存中， 在迭代中可以重复使
用； 虽然 boosting 算法迭代必须串行， 但是在处理每个特征列时可以做到并行。 4） 按照特征列
方式存储能优化寻找最佳的分割点， 但是当以行计算梯度数据时会导致内存的不连续访问， 严重
时会导致 cache miss， 降低算法效率。 paper 中提到， 可先将数据收集到线程内部的 buffer，
然后再计算， 提高算法的效率。 5） xgboost 还考虑了当数据量比较大， 内存不够时怎么有效的
使用磁盘， 主要是结合多线程、 数据压缩、 分片的方法， 尽可能的提高算法的效率。
适用场景： 分类回归问题都可以。
Rf：
优点：
1） 表现性能好， 与其他算法相比有着很大优势。
2） 随机森林能处理很高维度的数据（也就是很多特征的数据） ， 并且不用做特征选择。
3） 在训练完之后， 随机森林能给出哪些特征比较重要。
4） 训练速度快， 容易做成并行化方法(训练时， 树与树之间是相互独立的) 。
5） 在训练过程中， 能够检测到 feature 之间的影响。
6） 对于不平衡数据集来说， 随机森林可以平衡误差。 当存在分类不平衡的情况时， 随机森
林能提供平衡数据集误差的有效方法。
7） 如果有很大一部分的特征遗失， 用 RF 算法仍然可以维持准确度。
8） 随机森林算法有很强的抗干扰能力（具体体现在 6, 7 点） 。 所以当数据存在大量的数据
缺失， 用 RF 也是不错的。

9） 随机森林抗过拟合能力比较强（虽然理论上说随机森林不会产生过拟合现象， 但是在现
实中噪声是不能忽略的， 增加树虽然能够减小过拟合， 但没有办法完全消除过拟合， 无论怎么增
加树都不行， 再说树的数目 也不可能无限增加的） 。
10） 随机森林能够解决分类与回归两种类型的问题， 并在这两方面都有相当好的估计表现。
（虽然 RF 能做回归问题， 但通常都用 RF 来解决分类问题） 。
11） 在创建随机森林时候， 对 generlization error(泛化误差) 使用的是无偏估计模型， 泛
化能力强。
缺点：
1） 随机森林在解决回归问题时， 并没有像它在分类中表现的那么好， 这是因为它并不能给
出一个连续的输出。 当进行回归时， 随机森林不能够做出超越训练集数据范围的预测， 这可能导
致在某些特定噪声的数据进行建模时出现过度拟合。 （PS: 随机森林已经被证明在某些噪音较大
的分类或者回归问题上回过拟合） 。
2） 对于许多统计建模者来说， 随机森林给人的感觉就像一个黑盒子， 你无法控制模型内部
的运行。 只能在不同的参数和随机种子之间进行尝试。
3） 可能有很多相似的决策树， 掩盖了真实的结果。
4） 对于小数据或者低维数据（特征较少的数据） ， 可能不能产生很好的分类。 （处理高维
数据， 处理特征遗失数据， 处理不平衡数据是随机森林的长处） 。
5） 执行数据虽然比 boosting 等快（随机森林属于 bagging） ， 但比单只决策树慢多了。
适用场景： 数据维度相对低（几十维） ， 同时对准确性有较高要求时。 因为不需要很多参数
调整就可以达到不错的效果， 基本上不知道用什么方法的时候都可以先试一下随机森林。
Lr：
优点： 实现简单， 广泛的应用于工业问题上； 分类时计算量非常小， 速度很快， 存储资源低；
便利的观测样本概率分数； 对逻辑回归而言， 多重共线性并不是问题， 它可以结合 L2 正则化来
解决该问题。
缺点： 当特征空间很大时， 逻辑回归的性能不是很好； 容易欠拟合， 一般准确度不太高
不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的
softmax 可以用于多分类） ， 且必须线性可分； 对于非线性特征， 需要进行转换。
适用场景： LR 同样是很多分类算法的基础组件， 它的好处是输出值自 然地落在 0 到 1 之间，
并且有概率意义。 因为它本质上是一个线性的分类器， 所以处理不好特征之间相关的情况。 虽然
效果一般， 却胜在模型清晰， 背后的概率学经得住推敲。 它拟合出来的参数就代表了每一个特征
(feature) 对结果的影响。 也是一个理解数据的好工具。

50、xgboost 特征并行化怎么做的

考点: xgboost 并行化
标签： XGBoost
参考回答：
决策树的学习最耗时的一个步骤就是对特征值进行排序, 在进行节点分裂时需要计算每个特
征的增益, 最终选增益大的特征做分裂, 各个特征的增益计算就可开启多线程进行。 而且可以采用
并行化的近似直方图算法进行节点分裂。

51、xgboost 和 lightgbm 的区别和适用场景

标签： XGBoost
参考回答: （1） xgboost 采用的是 level-wise 的分裂策略， 而 lightGBM 采用了 leaf-wise
的策略， 区别是 xgboost 对每一层所有节点做无差别分裂， 可能有些节点的增益非常小， 对结果
影响不大， 但是 xgboost 也进行了分裂， 带来了务必要的开销。 leaft-wise 的做法是在当前所
有叶子节点中选择分裂收益最大的节点进行分裂， 如此递归进行， 很明显 leaf-wise 这种做法容
易过拟合， 因为容易陷入比较高的深度中， 因此需要对最大深度做限制， 从而避免过拟合。
（2） lightgbm 使用了基于 histogram 的决策树算法， 这一点不同与 xgboost 中的 exact 算
法， histogram 算法在内存和计算代价上都有不小优势。 1） 内存上优势： 很明显， 直方图算法
的内存消耗为(#data* #features * 1Bytes) (因为对特征分桶后只需保存特征离散化之后的值) ，
而 xgboost 的 exact 算法内存消耗为： (2 * #data * #features* 4Bytes) ， 因为 xgboost 既要
保存原始 feature 的值， 也要保存这个值的顺序索引 ， 这些值需要 32 位的浮点数来保存。 2） 计
算上的优势， 预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值， 时间为
(#data) , 而直方图算法只需要遍历桶就行了， 时间为(#bin)
（3） 直方图做差加速， 一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图
得到， 从而加速计算。
（4） lightgbm 支持直接输入 categorical 的 feature， 在对离散特征分裂时， 每个取值都
当作一个桶， 分裂时的增益算的是” 是否属于某个 category“的 gain。 类似于 one-hot 编码。
（5） xgboost 在每一层都动态构建直方图， 因为 xgboost 的直方图算法不是针对某个特定的
feature， 而是所有 feature 共享一个直方图(每个样本的权重是二阶导) , 所以每一层都要重新构
建直方图， 而 lightgbm 中对每个特征都有一个直方图， 所以构建一次直方图就够了。
其适用场景根据实际项目 和两种算法的优点进行选择。

52、HMM 隐马尔可夫模型的参数估计方法是？（EM算法）

53、Bootstrap 方法是什么？

知识点： Bootstap、 集成学习
详情： Bagging
答案： 从一个数据集中有放回的抽取 N 次， 每次抽 M 个。
解析： Bagging 算法基于 bootstrap。 面试时结合 Bagging 算法讲述会更好。

54、如何防止过拟合？

知识点： 过拟合
详情: Bagging
答案： 1. 早停法； 2. l1 和 l2 正则化； 3. 神经网络的 dropout； 4. 决策树剪枝； 5. SVM 的松
弛变量； 6. 集成学习
解析： 能够达到模型权重减小， 模型简单的效果

55、EM 算法推导， jensen 不等式确定的下界

p84

p104

# 深度学习面试题

## 神经网络基础

神经网络为啥用交叉熵。

通过神经网络解决多分类问题时， 最常用的一种方式就是在最后一层设置 n 个输出节点， 无
论在浅层神经网络还是在 CNN 中都是如此， 比如， 在 AlexNet 中最后的输出层有 1000 个节点，
而即便是 ResNet 取消了全连接层， 也会在最后有一个 1000 个节点的输出层。
一般情况下， 最后一个输出层的节点个数与分类任务的目 标数相等。 假设最后的节点数为 N，
那么对于每一个样例， 神经网络可以得到一个 N 维的数组作为输出结果， 数组中每一个维度会对
应一个类别。 在最理想的情况下， 如果一个样本属于 k， 那么这个类别所对应的的输出节点的输
出值应该为 1， 而其他节点的输出都为 0， 即[0, 0, 1, 0, …. 0, 0] ， 这个数组也就是样本的 Label，
是神经网络最期望的输出结果， 交叉熵就是用来判定实际的输出与期望的输出的接近程度。

DNN 的梯度更新方式

神经网络模型怎么防止过拟合？ 招银 字节跳动

5、 激活函数的作用
考点: 激活函数
参考回答：
激活函数是用来加入非线性因素的, 提高神经网络对模型的表达能力, 解决线性模型所不能
解决的问题。

36、 为什么用 relu 就不用 sigmoid 了
考点: 激活函数
标签： 深度学习
参考回答：
Sigmoid 的导数只有在 0 的附近时有比较好的激活性, 在正负饱和区域的梯度都接近 0， 会导
致梯度弥散。 而 relu 函数在大于 0 的部分梯度为常数, 不会产生梯度弥散现象。 Relu 函数在负
半区导数为 0, 也就是说这个神经元不会经历训练, 就是所谓稀疏性。 而且 relu 函数的导数计算
的更快。

2、梯度消失

考点： 深度学习
参考回答： 在神经网络中， 当前面隐藏层的学习速率低于后面隐藏层的学习速率， 即随着隐
藏层数目 的增加， 分类准确率反而下降了。 这种现象叫做消失的梯度问题。

神经网络防止梯度消失的方法。通联数据

1） 、 使用 ReLU、 LReLU、 ELU、 maxout 等激活函数
sigmoid 函数的梯度随着 x 的增大或减小和消失， 而 ReLU 不会。
2） 、 使用批规范化
通过规范化操作将输出信号 x 规范化到均值为 0， 方差为 1 保证网络的稳定性。 从上述分析
分可以看到， 反向传播式子中有 w 的存在， 所以 w 的大小影响了梯度的消失和爆炸， Batch
Normalization 就是通过对每一层的输出规范为均值和方差一致的方法， 消除了 w 带来的放大缩
小的影响， 进而解决梯度消失和爆炸的问题。

1、Batch Normalization 的作用 字节跳动

这一个问题其实我有用过，我也知道原理，但是当面试问具体公示的时候，还是愣了一下，但是BN无疑是一个很火的应用，必须要会。 BN前向反向都要看看

考点: 神经网络基础
详情： 深度学习
参考回答：
神经网络在训练的时候随着网络层数的加深, 激活函数的输入值的整体分布逐渐往激活函数
的取值区间上下限靠近, 从而导致在反向传播时低层的神经网络的梯度消失。 而
Batch Normalization 的作用是通过规范化的手段, 将越来越偏的分布拉回到标准化的分布, 使得
激活函数的输入值落在激活函数对输入比较敏感的区域, 从而使梯度变大, 加快学习收敛速度, 避
免梯度消失的问题。

6、训练过程中, 若一个模型不收敛, 那么是否说明这个模型无效?导致模型不收敛的原因有
哪些?

考点: 神经网络基础
详情： 深度学习
参考回答：
并不能说明这个模型无效, 导致模型不收敛的原因可能有数据分类的标注不准确, 样本的信
息量太大导致模型不足以 fit 整个样本空间。 学习率设置的太大容易产生震荡, 太小会导致不收
敛。 可能复杂的分类任务用了简单的模型。 数据没有进行归一化的操作。

9、 Relu 比 Sigmoid 的效果好在哪里?
考点: 激活函数对比
详情： 深度学习
参考回答：
Sigmoid 的导数只有在 0 的附近时有较好的激活性, 而在正负饱和区域的梯度趋向于 0, 从而
产生梯度弥散的现象, 而 relu 在大于 0 的部分梯度为常数, 所以不会有梯度弥散现象。 Relu 的导
数计算的更快。 Relu 在负半区的导数为 0, 所以神经元激活值为负时, 梯度为 0, 此神经元不参与
训练, 具有稀疏性。

10、 问题： 神经网络中权重共享的是？
知识点： 权重共享
详情： 深度学习
参考回答： 卷积神经网络、 循环神经网络
解析： 通过网络结构直接解释

16、 什么是 dropout,dropout 咋回事讲讲 字节跳动
考点: 防过拟合方法
详情： 深度学习
参考回答：
在神经网络的训练过程中, 对于神经单元按一定的概率将其随机从网络中丢弃, 从而达到对
于每个 mini-batch 都是在训练不同网络的效果, 防止过拟合。
Dropout 的目 标是在指数 级数量的神经网络上近似这个过程。Dropout 训练与 Bagging 训练
不太一样。 在 Bagging 的情况下, 所有模型是独立的。
在 Dropout 的情况下, 模型是共享参数的, 其中每个模型继承的父神经网络参 数的不同子集。
参数共享使得在有限可用的内存下代表指数数量的模型变得可能。 在 Bagging 的情况下, 每一个
模型在其相应训练集上训练到收敛。
在 Dropout 的情况下, 通常大部分模型都没有显式地被训练, 通常该模型很大, 以致到宇
宙毁灭都不 能采样所有可能的子网络。 取而代之的是, 可能的子网络的一小部分训练单个步骤,
参数共享导致剩余的子网络能有好的参数设定。



## 神经网络优化方法

随机梯度下降法的原理？招银

用过哪些 Optimizer， 效果如何

1） SGD； 2） Momentum； 3） Nesterov； 4） Adagrad； 5） Adadelta； 6） RMSprop； 7） Adam；
8） Adamax； 9） Nadam。 

（1） 对于稀疏数据， 尽量使用学习率可自 适应的算法， 不用手动调节，
而且最好采用默认参数。 （2） SGD 通常训练时间最长， 但是在好的初始化和学习率调度方案下，
结果往往更可靠。 但 SGD 容易困在鞍点， 这个缺点也不能忽略。 （3） 如果在意收敛的速度， 并
且需要训练比较深比较复杂的网络时， 推荐使用学习率自 适应的优化方法。（4）Adagrad， Adadelta
和 RMSprop 是比较相近的算法， 表现都差不多。 （5） 在能使用带动量的 RMSprop 或者 Adam 的地
方， 使用 Nadam 往往能取得更好的效果。

## CNN

39、 SPP， YOLO 了解吗？
标签： 深度学习
参考回答：
SPP-Net 简介：
SPP-Net 主要改进有下面两个：
1） . 共享卷积计算、 2） . 空间金字塔池化
在 SPP-Net 中同样由这几个部分组成：
ss 算法、 CNN 网络、 SVM 分类器、 bounding box
ss 算法的区域建议框同样在原图上生成， 但是却在 Conv5 上提取， 当然由于尺寸的变化，
在 Conv5 层上提取时要经过尺度变换， 这是它 R-CNN 最大的不同， 也是 SPP-Net 能够大幅缩短时
长的原因。 因为它充分利用了卷积计算， 也就是每张图片只卷积一次， 但是这种改进带来了一个
新的问题， 由于 ss 算法生成的推荐框尺度是不一致的， 所以在 cov5 上提取到的特征尺度也是不
一致的， 这样是没有办法做全尺寸卷积的（Alexnet） 。
所以 SPP-Net 需要一种算法， 这种算法能够把不一致的输入产生统一的输出， 这就 SPP， 即
空间金字塔池化， 由它替换 R-CNN 中的 pooling 层， 除此之外， 它和 R-CNN 就一样了。
YOLO 详解：
YOLO 的名字 You only look once 正是自 身特点的高度概括。 YOLO 的核心思想在于将目 标检
测作为回归问题解决 ， YOLO 首先将图片划分成 SxS 个区域， 注意这个区域的概念不同于上文提
及将图片划分成 N 个区域扔进 detector 这里的区域不同。 上文提及的区域是真的将图片进行剪
裁， 或者说把图片的某个局部的像素扔进 detector， 而这里的划分区域， 只的是逻辑上的划分。

18、 HOG 算法原理
考点: 目 标检测算法
详情: 深度学习

7、 图像处理中锐化和平滑的操作

考点: 图像处理基础
详情： 深度学习
参考回答：
锐化就是通过增强高频分量来减少图像中的模糊, 在增强图像边缘的同时也增加了图像的噪
声。
平滑与锐化相反, 过滤掉高频分量, 减少图像的噪声是图片变得模糊。

8、VGG 使用 3*3 卷积核的优势是什么?

考点: VGG 基础
详情： 深度学习
参考回答：
2 个 3*3 的卷积核串联和 5*5 的卷积核有相同的感知野, 前者拥有更少的参数。 多个 3*3 的
卷积核比一个较大尺寸的卷积核有更多层的非线性函数, 增加了非线性表达, 使判决函数更具有
判决性。





12、 问题： 在深度学习中， 通常会 finetuning 已有的成熟模型， 再基于新数据， 修改最后
几层神经网络权值， 为什么？
知识点： finetuning， 迁移学习
详情： 深度学习
参考回答： 实践中的数据集质量参差不齐， 可以使用训练好的网络来进行提取特征。 把训
练好的网络当做特征提取器。

3、循环神经网络， 为什么好?

考点： 深度学习
参考回答： 循环神经网络模型（RNN） 是一种节点定向连接成环的人工神经网络， 是一种反
馈神经网络， RNN 利用内部的记忆来处理任意时序的输入序列， 并且在其处理单元之间既有内部
的反馈连接又有前馈连接， 这使得 RNN 可以更加容易处理不分段的文本等。

4、什么是 Group Convolution

考点: 卷积的应用
详情： 深度学习
参考回答：
若卷积神将网络的上一层有 N 个卷积核, 则对应的通道数也为 N。 设群数目 为 M, 在进行卷积
操作的时候, 将通道分成 M 份, 每个 group 对应 N/M 个通道, 然后每个 group 卷积完成后输出叠在
一起, 作为当前层的输出通道。

31、 GAN 网络的思想
考点: GAN
标签： 深度学习
参考回答：
GAN 用一个生成模型和一个判别模型, 判别模型用于判断给定的图片是不是真实的图片, 生
成模型自 己生成一张图片和想要的图片很像, 开始时两个模型都没有训练, 然后两个模型一起进
行对抗训练, 生成模型产生图片去欺骗判别模型, 判别模型去判别真假, 最终两个模型在训练过程
中, 能力越来越强最终达到稳态。

32、 1*1 的卷积作用
考点: CNN 基础
标签： 深度学习
参考回答：
实现跨通道的交互和信息整合, 实现卷积核通道数的降维和升维, 可以实现多个 feature map
的线性组合, 而且可是实现与全连接层的等价效果。

RetinaNet 为什么比 SSD 效果好

RetinaNet 的大致结构画一下

Depthwise 卷积实际速度与理论速度差距较大， 解释原因。

模型压缩的大方向。 CPM 模型怎么压缩的， 做了哪些工作？

介绍一下图像的高频、 低频部分， 知道哪些图像补全的方法

图像基础： 传统图像处理方法知道哪些， 图像对比度增强说一下

CNN 为什么比 DNN 在图像识别上更好

Inception Score 评价指标介绍

使用的 CNN 模型权重之间有关联吗？

权重之间有关联。 CNN 是权重共享， 减少了参数的数量。
简单来说就是用一个卷积核来和一个图像来进行卷积， 记住是同一个卷积核， 不改变卷积核
的值。 这样可以减少权值参数。 共享就是一个图片对卷积核是共同享有的。 对于一个 100*100
像素的图像， 如果我们用一个神经元来对图像进行操作， 这个神经元大小就是 100*100=10000，
单如果我们使用 10*10 的卷积核， 我们虽然需要计算多次， 但我们需要的参数只有 10*10=100
个， 加上一个偏向 b， 一共只需要 101 个参数。 我们取得图像大小还是 100*100。 如果我们取得
图像比较大， 它的参数将会更加多。 我们通过 10*10 的卷积核对图像进行特征提取， 这样我们就
得到一个 Feature Map。
一个卷积核只能提取一个特征， 所以我们需要多几个卷积核， 假设我们有 6 个卷积核， 我们
就会得到 6 个 Feature Map， 将这 6 个 Feature Map 组成一起就是一个神经元。这 6 个 Feature Map
我们需要 101*6=606 个参数。 这个值和 10000 比还是比较小的。 如果像之前的神经网络, 两两相
连, 需要 28x28 = 784 输入层, 加上第一个隐藏层 30 个神经元, 则需要 784x30 再加上 30 个 b,
总共 23, 550 个参数! 多了 40 倍的参数。

百度实习： 1） 模型压缩方法； 2） CPM 模型压缩用了哪些方法； 3） 压缩效果（体积、 指
标、 部署） ； 4） Kaggle 比赛， 比赛背景， 怎么进行数据清洗， 类别平衡， 相近类别重分类， 最
终成绩是多少， 觉得跟前几名差距在哪， 有没有尝试过集成的方法； 5） 人脸项目 ， 大概流程，
GPU 加速的地方， 两个网络的训练过程， 级联网络的 inference 过程， 能同时检测多个人脸吗？
多尺度缩放怎么处理， resize 自 己写？ 只是检测吗， 有没有识别？ 或者其他

CycleGAN 原理介绍一下

训练 GAN 的时候有没有遇到什么问题

CPM 模型压缩怎么做的？ 有压过 OpenPose 吗？

## RNN

LSTM 和 Naive RNN 的区别

Word2vec的两种模式？它的原理能讲讲吗？招银/通联

Word2vec原理？它是怎么学习词向量的？ 喜马拉雅

TF-IDF是什么？通联

Word-embedding的方式有哪些？ 通联 喜马拉雅

RNN 容易梯度消失， 怎么解决？

Bert知道吗？讲一下？通联

标签： 深度学习
参考回答：
1） 、 梯度裁剪（Clipping Gradient）
既然在 BP 过程中会产生梯度消失（就是偏导无限接近 0， 导致长时记忆无法更新） ， 那么
最简单粗暴的方法， 设定阈值， 当梯度小于阈值时， 更新的梯度为阈值。
优点： 简单粗暴
缺点： 很难找到满意的阈值
2） 、 LSTM（Long Short-Term Memory）
一定程度上模仿了长时记忆， 相比于梯度裁剪， 最大的优点就是， 自 动学习在什么时候可以
将 error 反向传播， 自 动控制哪些是需要作为记忆存储在 LSTM cell 中。 一般长时记忆模型包括
写入， 读取， 和忘记三个过程对应到 LSTM 中就变成了 input_gate, output_gate,
forget_gate, 三个门， 范围在 0 到 1 之间， 相当于对输入输出进行加权的学习， 利用大量数
据来自 动学习加权的参数（即学习了哪些错误可以用 BP 更新参数） 。 具体的公式表达：

优点： 模型自 动学习更新参数

LSTM 跟 RNN 有啥区别

标签： 深度学习
参考回答：
LSTM 与 RNN 的比较
RNN 在处理 long term memory 的时候存在缺陷， 因此 LSTM 应运而生。 LSTM 是一种变种的
RNN， 它的精髓在于引 入了细胞状态这样一个概念， 不同于 RNN 只考虑最近的状态， LSTM 的细胞
状态会决定哪些状态应该被留下来， 哪些状态应该被遗忘。
由上面两幅图可以观察到， LSTM 结构更为复杂， 在 RNN 中， 将过去的输出和当前的输入
concatenate 到一起， 通过 tanh 来控制两者的输出， 它只考虑最近时刻的状态。 在 RNN 中有两
个输入和一个输出。
而 LSTM 为了能记住长期的状态， 在 RNN 的基础上增加了一路输入和一路输出， 增加的这一
路就是细胞状态， 也就是途中最上面的一条通路。 事实上整个 LSTM 分成了三个部分：
1） 哪些细胞状态应该被遗忘
2） 哪些新的状态应该被加入
3） 根据当前的状态和现在的输入， 输出应该是什么

注意力公式

Attention(Query, Source) $=\sum_{i=1}^{L_{x}} a_{i} \cdot V a l u e_{i}$

Bert知道吗？讲一下？通联

给你很多文本，如何进行文本关键词的抓取？通联

设计一个算法，实现新闻搜索的推荐。要求：考虑时间复杂度和空间复杂度 通联

LSTM用过吗？如何设计一个LSTM结构，来完成命名实体识别任务？ 通联

Attention 字节跳动

  考虑到最近Attention的火爆和字节跳动的主要技术优势，attention这一个技术热点是一定要复习的（然而我没仔细看，非常后悔，面完之后从头到尾看了一遍）。 



5、什么是 RNN

考点: 循环神经网络
详情： 深度学习
参考回答：
一个序列当前的输出与前面的输出也有关, 在 RNN 网络结构中中, 隐藏层的输入不仅包括输
入层的输出还包含上一时刻隐藏层的输出, 网络会对之前的信息进行记忆并应用于当前的输入计
算中。





13、 问题： 画 GRU 结构图
知识点： GRU
详情： 深度学习

参考回答： GRU 有两个门： 更新们， 输出门
解析： 如果不会画 GRU， 可以画 LSTM 或者 RNN。 再或者可以讲解 GRU 与其他两个网络的联
系和区别。 不要直接就说不会。

14、 Attention 机制的作用
考点: 注意力机制基础
详情： 深度学习
参考回答：减少处理高维输入数据的计算负担, 结构化的选取输入的子集, 从而降低数据的维度。让系统
更加容易的找到输入的数据中与当前输出信息相关的有用信息, 从而提高输出的质量。 帮助类似
于 decoder 这样的模型框架更好的学到多种内容模态之间的相互关系。

Attention 简单理解就是权重分配， 。 以 seq2seq 中的 attention 公式作为讲解。 就是对输
入的每个词分配一个权重， 权重的计算方式为与解码端的隐含层时刻作比较， 得到的权重的意义
就是权重越大， 该词越重要。 最终加权求和。	

15、 Lstm 和 Gru 的原理
考点: 循环神经网络
详情： 深度学习
参考回答：
Lstm 由输入门, 遗忘门, 输出门和一个 cell 组成。 第一步是决定从 cell 状态中丢弃什么信
息, 然后在决定有多少新的信息进入到 cell 状态中, 最终基于目 前的 cell 状态决定输出什么样的
信息。
Gru 由重置门和跟新门组成, 其输入为前一时刻隐藏层的输出和当前的输入, 输出为下一时
刻隐藏层的信息。 重置门用来计算候选隐藏层的输出, 其作用是控制保留多少前一时刻的隐藏层。
跟新门的作用是控制加入多少候选隐藏层的输出信息, 从而得到当前隐藏层的输出。



17、 LSTM 每个门的计算公式
考点: 循环网络基础
详情： 深度学习
参考回答：



19、 DropConnect 的原理
考点: 防过拟合方法
详情： 深度学习
参考回答：
防止过拟合方法的一种, 与 dropout 不同的是, 它不是按概率将隐藏层的节点输出清 0, 而是
对每个节点与之相连的输入权值以一定的概率清 0。



22、 除了 GMM-HMM， 你了解深度学习在语音识别中的应用吗？

讲了我用的过 DNN-HMM， 以及与 GMM-HMM 的联系与区别； 然后 RNN+CTC， 这里我只是了解，
大概讲了一下 CTC 损失的原理； 然后提了一下 CNN+LSTM。

23、 用过哪些移动端深度学习框架？
参考回答：
开源的有： 小米的 MACE， 骁龙的 SNPE， 腾讯的 FeatherCNN 和 ncnn， 百度的
mobile-deep-learning(MDL) ； caffe、 tensorflow lite 都有移动端， 只是可能没有上面的
框架效率高。 据传还有支付宝的 xNN， 商汤的 PPL， 不过都是自 用， 未开源。

24、 Caffe： 整体架构说一下， 新加一个层需要哪些步骤， 卷积是怎么实现的， 多卡机制，
数据并行还是模型并行？

25、 HOG 算子是怎么求梯度的
参考回答：

26、 BN 层的作用， 为什么要在后面加伽马和贝塔， 不加可以吗标签: 深度学习
参考回答：
BN 层的作用是把一个 batch 内的所有数据， 从不规范的分布拉到正态分布。 这样做的好处
是使得数据能够分布在激活函数的敏感区域， 敏感区域即为梯度较大的区域， 因此在反向传播的
时候能够较快反馈误差传播。

27、 梯度消失， 梯度爆炸的问题，
标签： 深度学习
参考回答：
激活函数的原因， 由于梯度求导的过程中梯度非常小， 无法有效反向传播误差， 造成梯度消
失的问题。

28、 Adam
标签： 深度学习
参考回答：
Adam 算法和传统的随机梯度下降不同。 随机梯度下降保持单一的学习率（即 alpha） 更新
所有的权重， 学习率在训练过程中并不会改变。 而 Adam 通过计算梯度的一阶矩估计和二阶矩估
计而为不同的参数设计独立的自 适应性学习率。

30、 RNN 梯度消失问题,为什么 LSTM 和 GRU 可以解决此问题
考点: 循环网络模型
标签： 深度学习
参考回答：
RNN 由于网络较深, 后面层的输出误差很难影响到前面层的计算, RNN 的某一单元主要受它附
近单元的影响。 而 LSTM 因为可以通过阀门记忆一些长期的信息, 相应的也就保留了更多的梯度。
而 GRU 也可通过重置和更新两个阀门保留长期的记忆, 也相对解决了梯度消失的问题。



33、 怎么提升网络的泛化能力

考点: 网络性能提升
标签： 深度学习
参考回答：
从数据上提升性能: 收集更多的数据, 对数据做缩放和变换, 特征组合和重新定义问题。
从算法调优上提升性能: 用可靠的模型诊断工具对模型进行诊断, 权重的初始化, 用小的随机
数初始化权重。 对学习率进行调节, 尝试选择合适的激活函数, 调整网络的拓扑结构, 调节 batch
和 epoch 的大小, 添加正则化的方法, 尝试使用其它的优化方法, 使用 early stopping。

34、 什么是 seq2seq model
考点: 编码解码模型
标签： 深度学习
参考回答：
Seq2seq 属于 encoder-decoder 结构的一种, 利用两个 RNN, 一个作为 encoder 一个作为
decoder。 Encoder 负责将输入序列压缩成指定长度的向量, 这个向量可以看作这段序列的语义,
而 decoder 负责根据语义向量生成指定的序列。

3

37、 讲一下基于 WFST 的静态解码网络的语音识别流程？
参考回答：
从语音特征开始讲起， 我讲了 MFCC 和 LPC 的原理以及提取过程， 这一部分讲的很细， 然后
讲了 viterbi 解码过程， 最后概述了一下 HCLG. fst 构建流程
38、 目 标检测了解吗， Faster RCNN 跟 RCNN 有什么区别
标签： 深度学习
参考回答：
目 标检测， 也叫目 标提取， 是一种基于目 标几何和统计特征的图像分割， 它将目 标的分割和
识别合二为一， 其准确性和实时性是整个系统的一项重要能力。 尤其是在复杂场景中， 需要对多
个目 标进行实时处理时， 目 标自 动提取和识别就显得特别重要。
随着计算机技术的发展和计算机视觉原理的广泛应用， 利用计算机图像处理技术对目 标进行
实时跟踪研究越来越热门， 对目 标进行动态实时跟踪定位在智能化交通系统、 智能监控系统、 军
事目 标检测及医学导航手术中手术器械定位等方面具有广泛的应用价值。



40、 梯度消失梯度爆炸怎么解决
标签： 深度学习
参考回答：
1） 、 使用 ReLU、 LReLU、 ELU、 maxout 等激活函数
sigmoid 函数的梯度随着 x 的增大或减小和消失， 而 ReLU 不会。
2） 、 使用批规范化BN

通过规范化操作将输出信号 x 规范化到均值为 0， 方差为 1 保证网络的稳定性。 从上述分析
分可以看到， 反向传播式子中有 w 的存在， 所以 w 的大小影响了梯度的消失和爆炸， Batch
Normalization 就是通过对每一层的输出规范为均值和方差一致的方法， 消除了 w 带来的放大缩
小的影响， 进而解决梯度消失和爆炸的问题。

41、 RNN 容易梯度消失， 怎么解决？
标签： 深度学习
参考回答：
1） 、 梯度裁剪（Clipping Gradient）
既然在 BP 过程中会产生梯度消失（就是偏导无限接近 0， 导致长时记忆无法更新） ， 那么
最简单粗暴的方法， 设定阈值， 当梯度小于阈值时， 更新的梯度为阈值。
优点： 简单粗暴
缺点： 很难找到满意的阈值
2） 、 LSTM（Long Short-Term Memory）
一定程度上模仿了长时记忆， 相比于梯度裁剪， 最大的优点就是， 自 动学习在什么时候可以
将 error 反向传播， 自 动控制哪些是需要作为记忆存储在 LSTM cell 中。 一般长时记忆模型包括
写入， 读取， 和忘记三个过程对应到 LSTM 中就变成了 input_gate, output_gate,
forget_gate, 三个门， 范围在 0 到 1 之间， 相当于对输入输出进行加权的学习， 利用大量数
据来自 动学习加权的参数（即学习了哪些错误可以用 BP 更新参数） 。 具体的公式表达：
优点： 模型自 动学习更新参数

42、 LSTM 跟 RNN 有啥区别
标签： 深度学习
参考回答：

43、 卷积层和池化层有什么区别
标签： 深度学习
参考回答：

45、 dropout 和bagging
标签： 深度学习
参考回答：
Dropout 的目 标是在指数 级数量的神经网络上近似这个过程。Dropout 训练与 Bagging 训练
不太一样。 在 Bagging 的情况下, 所有模型是独立的。
在 Dropout 的情况下, 模型是共享参数的, 其中每个模型继承的父神经网络参 数的不同子集。
参数共享使得在有限可用的内存下代表指数数量的模型变得可能。 在 Bagging 的情况下, 每一个
模型在其相应训练集上训练到收敛。
在 Dropout 的情况下, 通常大部分模型都没有显式地被训练, 通常该模型很大, 以致到宇
宙毁灭都不 能采样所有可能的子网络。 取而代之的是, 可能的子网络的一小部分训练单个步骤,
参数共享导致剩余的子网络能有好的参数设定。

46、 relu
标签： 深度学习
参考回答： 在深度神经网络中， 通常使用一种叫修正线性单元(Rectified linear unit， ReLU）
作为神经元的激活函数。 ReLU 起源于神经科学的研究： 2001 年， Dayan、 Abott 从生物学角度模
拟出了脑神经元接受信号更精确的激活模型， 如下图：



50、 Flappy. Bird 开发者, 怎么利用 DNQ 方法强化学习你的游戏 AI
标签： 深度学习
参考回答：
强化学习是机器学习里面的一个分支。 它强调如何基于环境而行动， 以取得最大化的预期收
益。其灵感来源于心理学中的行为主义理论， 既有机体如何在环境给予的奖励或者惩罚的刺激下，
逐步形成对刺激的预期， 产生能够最大利益的习惯性行为。 结构简图如下：

因为强化学习考虑到了自 主个体、 环境、 奖励等因素， 所以很多人包括强化学习的研究者
Richard Sutton 都认为它是人工智能中最高层的模型， 其它深度学习、 机器学习模型都是它的
子系统。 在围棋界先后打败世界冠军的李世乭和柯洁额 alphaGo 就使用了强化学习模型， 也正是
这两次比赛， 把人工智能这个概念传递给了大众。 使用的是卷积神经网络结构。

51、 LeNet-5 结构

52、 推导 LSTM 正向传播和单向传播过程
标签： 深度学习
参考回答：
前向推导过程：

53、 LSTM 原理， 与 GRU 区别

p156

## TensorFlow

\7. TensorFlow中， CNN的卷积核怎么定义？
\8. TensorFlow中怎么定义一个占位符？同花顺

20、 深度学习了解多少， 有看过底层代码吗？ caffe, tf?
建议找一个项目 中常用的方法， 看一下底层代码逻辑
详情： 深度学习
参考回答：
略（腾
讯） 深度学习过拟合， 过拟合的解决方案
\1. 早停法； 2. l1 和 l2 正则化； 3. 神经网络的 dropout； 4. 决策树剪枝； 5. SVM 的松弛变量；
\6. 集成学习 网络bagging BatchNormalization

# 算法题

##  查找

手写二分查找

算法题， 单调函数求零点 (简单的二分法)

特别大的数据量， 实现查找， 排序

参考回答：
1） 位图法
位图法是我在编程珠玑上看到的一种比较新颖的方法， 思路比较巧妙效率也很高。
使用场景举例： 对 2G 的数据量进行排序， 这是基本要求。
数据： 1、 每个数据不大于 8 亿； 2、 数据类型位 int； 3、 每个数据最多重复一次。
内存： 最多用 200M 的内存进行操作。
首先对占用的内存进行判断， 每个数据不大于 8 亿， 那么 8 亿是一个什么概念呢。
**
1 byte = 8 bit（位）
1024 byte = 8*1024 bit = 1k
1024 k = 8*1024*1024 bit = 1M = 8388608 bit
**
也就是 1M=8388608 位
而位图法的基本思想就是利用一位代表一个数字， 例如 3 位上为 1, 则说明 3 在数据中出现
过， 若为 0， 则说明 3 在数据中没有出现过。 所以当题目 中出现每个数据最多重复一次这个条件
时， 我们可以考虑使用位图法来进行大数据排序。
那么假如使用位图法来进行这题的排序， 内存占用多少呢。 由题目 知道每个数据不大于 8
亿， 那么我们就需要 8 亿位， 占用 800000000/8388608=95M 的空间， 满足最多使用 200M 内存进
行操作的条件， 这也是这题能够使用位图法来解决的一个基础。
2） 堆排序法
堆排序是 4 种平均时间复杂度为 nlogn 的排序方法之一， 其优点在于当求 M 个数中的前 n
个最大数， 和最小数的时候性能极好。 所以当从海量数据中要找出前 m 个最大值或最小值， 而对
其他值没有要求时， 使用堆排序法效果很好。
使用场景： 从 1 亿个整数里找出 100 个最大的数
步骤：
（1） 读取前 100 个数字， 建立最大值堆。 （这里采用堆排序将空间复杂度讲得很低， 要排序
1 亿个数， 但一次性只需读取 100 个数字， 或者设置其他基数， 不需要 1 次性读完所有数据， 降
低对内存要求）
（2） 依次读取余下的数， 与最大值堆作比较， 维持最大值堆。 可以每次读取的数量为一个磁
盘页面， 将每个页面的数据依次进堆比较， 这样节省 IO 时间。
（3） 将堆进行排序， 即可得到 100 个有序最大值。
堆排序是一种常见的算法， 但了解其的使用场景能够帮助我们更好的理解它。
3） 较为通用的分治策略
分治策略师对常见复杂问题的一种万能的解决方法， 虽然很多情况下， 分治策略的解法都不
是最优解， 但是其通用性很强。 分治法的核心就是将一个复杂的问题通过分解抽象成若干个简单
的问题。
应用场景： 10G 的数据， 在 2G 内存的单台机器上排序的算法
我的想法， 这个场景既没有介绍数据是否有重复， 也没有给出数据的范围， 也不是求最大的
个数。 而通过分治虽然可能需要的 io 次数很多， 但是对解决这个问题还是具有一定的可行性
的。
步骤：
（1）从大数据中抽取样本， 将需要排序的数据切分为多个样本数大致相等的区间， 例如： 1-100，
101-300…
（2） 将大数据文件切分为多个小数据文件， 这里要考虑 IO 次数和硬件资源问题， 例如可
将小数据文件数设定为 1G（要预留内存给执行时的程序使用）
（3） 使用最优的算法对小数据文件的数据进行排序， 将排序结果按照步骤 1 划分的区间进行
存储
（4） 对各个数据区间内的排序结果文件进行处理， 最终每个区间得到一个排序结果的文件
（5） 将各个区间的排序结果合并。 通过分治将大数据变成小数据进行处理， 再合并。

## Hash

Hash 表处理冲突的方法

开放定址法
为产生冲突的地址 H(key)求得一个地址序列,,(  s  m − ) , 其中

  H key +  MOD m。 其中 m 为表的长度, 而增量有三种取值方法, 线性探
测再散列, 平方探测再散列, 随即探测再散列。
链地址法
将所有 Hash 地址相同的记录都链接在同一链表中
再 Hash 法
同时构造多个不同的 Hash 函数, 当产生冲突时, 计算另一个 Hash 函数地址直到不再发生冲突
为止。
建立公共溢出区
将 Hash 表分为基本表和溢出表, 若是与基本表发生冲突, 都放入溢出表。

9.算法题 求一个数组的众数 众数的个数>(数组长度)/2 字节跳动

一致性哈希



现场用 collabedit 写代码， 一个怪异的归并算法。。。 之前没遇到过， 直接把归并写出
来， 但是说复杂度太高， 优化了三遍还不行， 最后说出用小顶堆解决了。。。

## 树

1、 问题： 手撕代码， 根据前序， 中序创建二叉树。

2、 算法题： 从右边看被遮挡的二叉树， 求露出的 node

4、 算法题， trim 二叉搜索树

```c++
class Solution {
    public:
    TreeNode* trimBST(TreeNode* root, int L, int R) {
        if(root==NULL)
            return NULL;
        if(root->val<L)
            return trimBST(root->right, L, R) ;
        else if(root->val>R)
            return trimBST(root->left, L, R) ;
        else
        {
        root->left=trimBST(root->left, L, R) ;
        root->right=trimBST(root->right, L, R) ;
        }
        return root;
    }
} ;
```

5、 红黑树

## 排序

1、 对一千万个整数排序, 整数范围在[-1000, 1000] 间, 用什么排序最快?

考点: 排序算法
详情： 数据结构与算法
参考回答：
在以上的情景下最好使用计数排序, 计数排序的基本思想为在排序前先统计这组数中其
它数小于这个数的个数, 其时间复杂度为 O(n + k), 其中 n 为整数的个数, k 为所有数的范
围, 此场景下的 n ≫ k, 所以计数排序要比其他基于的比较排序效果要好。

2、 堆排序的思想

考点: 排序算法
详情： 数据结构与算法
参考回答：
将待排序的序列构成一个大顶堆, 这个时候整个序列的最大值就是堆顶的根节点, 将它
与末尾节点进行交换, 然后末尾变成了最大值, 然后剩余 n-1 个元素重新构成一个堆, 这样得
到这 n 个元素的次大值, 反复进行以上操作便得到一个有序序列。

3、 冒泡排序

4、 快速排序的最优情况

快速排序的最优情况是 Partition 每次划分的都很均匀, 当排序的元素为 n 个, 则递归树
的深度为 logn + 。 在第一次做 Partition 的时候需对所有元素扫描一遍, 获得的枢纽元
将所有元素一分为二, 不断的划分下去直到排序结束, 而在此情况下快速排序的最优时间复
杂度为 nlogn。

5、 抽了两道面试题目 两道。 8 个球， 1 个比较重， 天平， 几步找到重的？

参考回答： 两次。 分成 3-3-2。
详情： 数据结构与算法

6、 求一些数里的第 n 大的数

参考回答：
两种解法：
\1. 快排的思想， 满足前一部分为 k-1 个数， 第 k 个数就是第 k 大的
\2. 堆排序， 维护一个 n 大小的小顶堆。 比小顶堆的值大， 顶部出堆， 新值进堆。 最终遍
历后， 堆顶部的就是第 k 大的数。
注意数组的长度和 n 的比较， 即非法查询。

7、 算法题： topK 给出 3 种解法

1） 局部淘汰法 -- 借助“冒泡排序” 获取 TopK

2） 局部淘汰法 -- 借助数据结构"堆"获取 TopK

3） 分治法 -- 借助” 快速排序“方法获取 TopK

9、 说一下小顶堆的调整过程

10、 算法题： 2sum， 3sum

## 高级算法

1、 手撕代码： 以概率 p 生成 1、 概率 1-p 生成 0 的 rand 函数， 得到 0-1 等概率的 rand 函
数， 计算新的 rand 函数中： 调用一次， while 循环的期望次数

2、 Kruskal 算法的基本过程

考点: 最小代价生成树
详情： 数据结构与算法
参考回答：
Kruskal 算法是以边为主导地位, 始终选取当前可用的拥有最小权值的边, 所选择的边不
能构成回路。 首先构造一个只有 n 个顶点没有边的非连通图, 给所有的边按值以从小到大的
顺序排序, 选择一个最小权值边, 若该边的两个顶点在不同的连通分量上, 加入到有效边中,
否则舍去这条边, 重新选择权值最小的边, 以此重复下去, 直到所有顶点在同一连通分量上。

3、 BFS 和 DFS 的实现思想
考点: 搜索算法
标签： 数据结构与算法
参考回答：
BFS: (1) 顶点 v 入队列(2) 当队列为非空时继续执行否则停止(3) 从队列头取顶点 v, 查找
顶点 v 的所有子节点并依次从队列尾插入(4) 跳到步骤 2
DFS: (1) 访问顶点 v 并打印节点(2) 遍历 v 的子节点 w, 若 w 存在递归的执行该节点。

4、 关联规则具体有哪两种算法, 它们之间的区别
参考回答：
Apriori 和 FP-growth 算法
Apriori 多次扫描交易数据库, 每次利用候选频繁集产生频繁集, 而 FP-growth 则利用树
形结构, 无需产生候选频繁集而直接得到频繁集, 减少了扫描交易数据库的次数, 提高算法的
效率但是 Apriori 有较好的扩展性可用于并行计算。 一般使用 Apriori 算法进行关联分
析, FP-growth 算法来高效发现频繁项集。

5、 贪婪算法
公司： 美团
标签： 数据结构与算法
参考回答：
贪婪算法(贪心算法) 是指在对问题进行求解时， 在每一步选择中都采取最好或者最优
(即最有利) 的选择， 从而希望能够导致结果是最好或者最优的算法。 贪婪算法所得到的结果
往往不是最优的结果(有时候会是最优解) ， 但是都是相对近似(接近) 最优解的结果。 贪婪算
法并没有固定的算法解决框架， 算法的关键是贪婪策略的选择， 根据不同的问题选择不同的
策略。
必须注意的是策略的选择必须具备无后效性， 即某个状态的选择不会影响到之前的状态，
只与当前状态有关， 所以对采用的贪婪的策略一定要仔细分析其是否满足无后效性。
其基本的解题思路为： 1) 建立数学模型来描述问题; 2) 把求解的问题分成若干个子问
题; 3) 对每一子问题求解， 得到子问题的局部最优解; 4) 把子问题对应的局部最优解合成原来
整个问题的一个近似最优解。

6、 模拟退火， 蚁群对比
参考回答：
模拟退火算法: 退火是一个物理过程， 粒子可以从高能状态转换为低能状态， 而从低能
转换为高能则有一定的随机性， 且温度越低越难从低能转换为高能。 就是在物体冷却的过程
中， 物体内部的粒子并不是同时冷却下来的。 因为在外围粒子降温的时候， 里边的粒子还会
将热量传给外围粒子， 就可能导致局部粒子温度上升， 当然， 最终物体的温度还是会下降到
环境温度的。
先给出一种求最优解的方法， 在寻找一个最优解的过程中采取分段检索的方式， 在可行
解中随机找出来一个， 然后在这个解附近进行检索， 找到更加优化的解， 放弃原来的， 在新
得到的解附近继续检索， 当无法继续找到一个更优解的时候就认为算法收敛， 最终得到的是
一个局部最优解。
蚁群算法: 很显然， 就是模仿蚂蚁寻找食物而发明的算法， 主要用途就是寻找最佳路径。
先讲一下蚂蚁是怎么寻找食物， 并且在寻找到了食物后怎么逐渐确定最佳路径的。
事实上， 蚂蚁的目 光很短， 它只会检索它附近的很小一部分空间， 但是它在寻路过程中
不会去重复它留下信息素的空间， 也就是它不会往回走。 当一群蚂蚁遇到障碍物了之后， 它
们会随机分为两拨， 一波往左一波往右， 但是因为环境会挥发掉它们的信息素， 于是， 较短
的路留下的信息素多， 而较长的路因为挥发较多， 也就留下得少了。 接下来， 蚁群就会趋向
于行走信息素较多的路径， 于是大部分蚂蚁就走了较短的路了。 但是蚁群中又有一些特别的
蚂蚁， 它们并不会走大家走的路， 而是以一定概率寻找新路， 如果新路更短， 信息素挥发更
少， 渐渐得蚁群就会走向更短的路径。
以上就是蚂蚁寻路的具体过程。 把这个过程映射到计算机算法上， 计算机在单次迭代过
程中， 会在路径的节点上留下信息素（可以使用数据变量来表示） 。 每次迭代都做信息素蒸
发处理， 多次迭代后聚集信息素较多的路径即可认为是较优路径。

7、 算法题： 名人问题， 给出最优解法
参考回答：
问题描述：
有 n 个人他们之间认识与否用邻接矩阵表示(1 表示认识， 0 表示不认识) ， 并 A 认识 B
并不意味着 B 认识 A， 也就意味着是个有向图。 如果一个人是名人， 他必须满足两个条件，
一个是他不认识任何人， 另一个是所有人必须都认识他。
解决问题：
用一个数组保存所有没检查的人的编号， 遍历数组中的两个人 i， j。 如果 i 认识 j， 则
i 必定不是名人， 删除 i， 如果 i 不认识 j， 则 j 必定不是名人， 删除 j， 最终会只剩下一个
人， 我们检查一下这个人是否是名人即可， 如果是， 返回这个人， 如果不是， 那么这 n 个人
中并无名人。

8、 代码题： 股票最大值。买卖股票的最佳时机

9、 编辑距离

## 链表

1、 如何判断单链表是否是循环链表

2、 反转链表

4、 算法题， 单链表判断是否有环 (leetcode easy) ， 以及判断环入口

## 数组

1、 找出数组中只出现 1 次的数， 其余数均出现 2 次， 扩展， 其余数出现 2 次以上

## 动态规划

1、 最短描述数， 10 的最短描述数是 3^2+1^2 所以是 2， 求一个数的最短描述数

2、 跳台阶问题， 每次只能跳 1 个台阶或者 2 个台阶， n 个台阶共有多少种方式

3、 动态规划和带记忆递归的区别

顶而下和自 底而上

4、 手撕代码： 0-1 矩阵的最大正方形

# 编程语言工作和环境

p188-p202





